{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression (Binary)\n",
    "## Language: Julia\n",
    "### Author: Daisy Nsibu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we want to predict whether or not a student will be admitted to a University based off of their GMAT Scores and their GPA. The GMAT score and their GPA is just one part of many pieces of the application process. So this is not going to definetly say whether or not a student will be admitted but what we can do is seek a likelihood as to whether a student is either admitted or not admitted. So this is a Binary decison, admitted or not admitted to a University. \n",
    "\n",
    "\n",
    "So what I am going to be traing my machine learning model to find is a way to predict the probability that a student will be admitted to this University.\n",
    "\n",
    "So, what I'll be be doing in this notebook is try to model logistic regression as a very simple neural network.\n",
    "\n",
    "### Feed Forward (with a single Neuron)\n",
    "\n",
    "Suppose for the example given above, that we have **N** data points.\n",
    "\n",
    "$$ (x^1, y^1), ..., (x^N, y^N)$$\n",
    "\n",
    "So here our feature vectors are going to consist of a student's:\n",
    " + GMAT score\n",
    " + GPA\n",
    " \n",
    "So it's a two-dimensional vector, $x$. And $y$ is going to be either a 0 or 1. \n",
    "\n",
    "$$x^i = [GMAT score, GPA]$$\n",
    "$$y^i = {(0, 1)}$$\n",
    "\n",
    "where [0 = not admitted, 1=admitted]\n",
    "\n",
    "So we have a feature vector ($x$) and a label ($y$) for each of these data points. Thus making this a supervised learning situation.\n",
    "\n",
    "We wish to predict either a 0 or 1. The way I am going to do that is by assigning probability given a feature vector, and if that feature vector is greater than 0.5 then we'll assgin it 1 and less than 0.5 well assign it a 0.\n",
    "\n",
    "So in order to get out this prediction for a given feature vector $x^i$ we're going to envision this as a single neuron.\n",
    "\n",
    "![](https://joshuagoings.com/assets/logistic.png)\n",
    "So we have two features for each vector. $x_1^{i}$ first feature, which is the gmat score and then  $x_2^{i}$ the second feature, which is the GPA.Then we also have the bias, which we're going to feed in as 1. \n",
    "\n",
    "Then $\\omega_1$ (weight 1), $\\omega_2$ (weight 2), and a bias $b$.\n",
    "\n",
    "We are going to say that the input to this neuron is Z. \n",
    "But what is Z? \n",
    "\n",
    "Z is just a linear combination of the the weights and the feature vectors.\n",
    "\n",
    "$$x_1^{i} * \\omega_1$$\n",
    "$$x_2^{i} * \\omega_2$$\n",
    "$$1*b$$\n",
    "\n",
    "Added all together they will look like, \n",
    "$$ Z^i= \\omega^T * x^i + b$$\n",
    "\n",
    "Now the value of Z is not going to be a probability. This value, *b* can range from - $\\infty$ to  $\\infty$ based off of various biases and weights you pick. However, there are ways to crunch  this number (*b*) into a value between 0 and 1. The way we're going to do that is by using the **sigmoid function**, and that is going to give us our prediction for the $i^{th}$ input. We'll call that $\\hat{y}^i$ . \n",
    "\n",
    "$\\hat{y}^i$ is the sigmoid function applied to $Z^i$\n",
    "\n",
    "$$\\hat{y}^i = \\sigma(Z^i) = \\sigma(\\omega^T * x^i + b) = \\frac{1}{1+e^{-\\omega^T * x^i + b}} $$\n",
    "\n",
    "The sigmoid function is,  $\\sigma(x)= \\frac{1}{1+e^{-x}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And if we plot this function, we see that all the values are between 0 an 1 for all real numbers.\n",
    "\n",
    "So what this ends up giving us here is that, for a given feature vector i, we feed the features into the neuron, take a linear combination of those features as Z, and then we choose a function after the fact to feed that linear combination into to get a hypothesis (or output).\n",
    "\n",
    "So Y here is implies a  hypothesis for feature vector $x^i$ because it's a probability. And if that probability is high enough then we can say that its going to be a one and if it's low enough we can just say that it's a zero. So,  it does imply a hypothesis or a prediction for $x^{i}$.\n",
    "\n",
    "so we're calling this feed forward, we are feeding in for a given entry."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Loss function\n",
    "\n",
    "We have a way to predict a probability that's based off of the weights and the bias. So our next goal is to write a function in terms of the weights and the bias, that if we minimize  makes the predictions better on the training data set. \n",
    "\n",
    "So our goal is to write a loss function in terms of the weights and biases. That if we minimize, we'll be minimizing according to this being close to the actual desired output.\n",
    "\n",
    "We want something like a loss function that compares $\\hat{y}$ to y for a given input $i$,\n",
    "\n",
    "$$ L(\\hat{y}^{i},{y}^{i})$$\n",
    "\n",
    "it somehow measures how close $\\hat{y}^{i}$ is to ${y}^{i}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to derive such a thing, lets first consider maximizing $P(y^{i}|x^{i})$, the probability that $\\hat{y}^{i}$ predicts ${y}^{i}$ ,the probability that these are close together.\n",
    "\n",
    "The prediction = {1 if $\\hat{y}^{i}$ $\\geqq$ 0.5, 0 otherwise}\n",
    "\n",
    "So in this probability space there are only two discrete outputs, 0 or 1.\n",
    "\n",
    "Since there are only two discrete outputs in this proability space, this is subject to the following formula by Bernoulli\n",
    "\n",
    " $$P(y^{i}|x^{i}) = \\hat{y}^{y}(1- \\hat{y})^{1-y}$$\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Deriving the Loss Function\n",
    "*(Note: I will drop the superscripts but know that this is for a single given entry)*\n",
    "\n",
    "$$maximize  \\longrightarrow P(y^{i}|x^{i}) = \\hat{y}^{y}(1- \\hat{y})^{1-y}$$ *(1)*\n",
    "\n",
    "now what we want to do is to maximize this probability to be the correct output. \n",
    "\n",
    "So what we're going to do is to take the logarithm on both sides.  \n",
    "\n",
    "$$ \\log P(y|x) = \\log [\\hat{y}^{y}(1 - \\hat{y})^{1-y}]$$ *(2)*\n",
    "\n",
    "\n",
    "So maximizing *(1)* is equivalent to maximizing *(2)*, they should both have the same min and max for arguements.\n",
    "\n",
    "Using our rules for logarithms we get,\n",
    "\n",
    "$\\log P(y|x) =  y \\log \\hat{y} + (1 - y) \\log (1 - \\hat{y})$ *(3)*\n",
    "\n",
    "but what we want is a minimization problem . We want to maximize *(3)* but for our algorithms ,for our gradient\n",
    "descent, we want a minimization problem so we can just multiply both sides of *(3)* by -1 and then we want a\n",
    "\n",
    "minimization problem.In other words, maximizing *(3)* is equivalent to minimizing the negative of it .\n",
    "\n",
    "$ -\\log P(y|x) =  -[y \\log \\hat{y} + (1 - y) \\log (1 - \\hat{y})]$  *(Cross-Entropy Loss function)* *(4)*\n",
    "\n",
    "$=  -[y \\log \\sigma(z) + (1 - y) \\log (1 - \\sigma(z))]$\n",
    "\n",
    "$ L_{ce} (\\omega, b)=  -[y \\log \\sigma(\\omega^{T } x + b) + (1 - y) \\log (1 - \\sigma(\\omega^{T} x + b))]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4. Train using stochastic Gradient Descent\n",
    "We want $$\\hat{\\theta} = \\underset{\\theta}{argmin} L_{ce}(x,y;\\theta)$$  where $\\theta = \\omega, b $\n",
    "\n",
    " $(1)$ \n",
    "$$ \\frac{\\partial L_{ce}}{\\partial \\omega_j}(\\omega, b) = [\\sigma(\\omega^{T}x + b) - y ]x_j $$\n",
    "\n",
    " $(2)$ \n",
    "$$ \\frac{\\partial L_{ce}}{\\partial b}(\\omega,b) = [\\sigma(\\omega^{T } x + b) - y ]$$\n",
    "\n",
    "This gives us is all the info needed to find the gradient.\n",
    "\n",
    "1. We randomly pick a point of data\n",
    "\n",
    "For a given data point ${x^{i}, y^{i}} , \n",
    "\n",
    "$$\\omega_j^{k+1} = \\omega_j^{k} - \\alpha * \\frac{\\partial L_{ce}}{\\partial \\omega_{j}^{k}}(\\omega^{k},b^{k})$$ \n",
    "\n",
    "at the k iteration step ( for j=1,2)\n",
    "\n",
    "$$b^{k+1} = b^{k} -\\alpha*\\frac{\\partial L_{ce}}{\\partial b}(w^{k}, b^{k})\n",
    "$$\n",
    "\n",
    "We calculate the gradient according to the rules of (1) and (2) and  update with the partial derivatives and then we iterate this until we see that the cross entropy loss is decreasing sufficiently\n",
    "\n",
    "*(Note: The *k* represents the k iteration ,the superscript *i* just represents the ith data points, the *j* here just represents either feature 1 or feature 2 from vector i)*\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Packages\n",
    "using Pkg\n",
    "using Plots\n",
    "using Random\n",
    "using CSV\n",
    "using DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"data-frame\"><thead><tr><th></th><th>gmat</th><th>gpa</th><th>work_experience</th><th>admitted</th></tr><tr><th></th><th>Int64</th><th>Float64</th><th>Int64</th><th>Int64</th></tr></thead><tbody><p>40 rows × 4 columns</p><tr><th>1</th><td>780</td><td>4.0</td><td>3</td><td>1</td></tr><tr><th>2</th><td>750</td><td>3.9</td><td>4</td><td>1</td></tr><tr><th>3</th><td>690</td><td>3.3</td><td>3</td><td>0</td></tr><tr><th>4</th><td>710</td><td>3.7</td><td>5</td><td>1</td></tr><tr><th>5</th><td>680</td><td>3.9</td><td>4</td><td>0</td></tr><tr><th>6</th><td>730</td><td>3.7</td><td>6</td><td>1</td></tr><tr><th>7</th><td>690</td><td>2.3</td><td>1</td><td>0</td></tr><tr><th>8</th><td>720</td><td>3.3</td><td>4</td><td>1</td></tr><tr><th>9</th><td>740</td><td>3.3</td><td>5</td><td>1</td></tr><tr><th>10</th><td>690</td><td>1.7</td><td>1</td><td>0</td></tr><tr><th>11</th><td>610</td><td>2.7</td><td>3</td><td>0</td></tr><tr><th>12</th><td>690</td><td>3.7</td><td>5</td><td>1</td></tr><tr><th>13</th><td>710</td><td>3.7</td><td>6</td><td>1</td></tr><tr><th>14</th><td>680</td><td>3.3</td><td>4</td><td>0</td></tr><tr><th>15</th><td>770</td><td>3.3</td><td>3</td><td>1</td></tr><tr><th>16</th><td>610</td><td>3.0</td><td>1</td><td>0</td></tr><tr><th>17</th><td>580</td><td>2.7</td><td>4</td><td>0</td></tr><tr><th>18</th><td>650</td><td>3.7</td><td>6</td><td>1</td></tr><tr><th>19</th><td>540</td><td>2.7</td><td>2</td><td>0</td></tr><tr><th>20</th><td>590</td><td>2.3</td><td>3</td><td>0</td></tr><tr><th>21</th><td>620</td><td>3.3</td><td>2</td><td>1</td></tr><tr><th>22</th><td>600</td><td>2.0</td><td>1</td><td>0</td></tr><tr><th>23</th><td>550</td><td>2.3</td><td>4</td><td>0</td></tr><tr><th>24</th><td>550</td><td>2.7</td><td>1</td><td>0</td></tr><tr><th>25</th><td>570</td><td>3.0</td><td>2</td><td>0</td></tr><tr><th>26</th><td>670</td><td>3.3</td><td>6</td><td>1</td></tr><tr><th>27</th><td>660</td><td>3.7</td><td>4</td><td>1</td></tr><tr><th>28</th><td>580</td><td>2.3</td><td>2</td><td>0</td></tr><tr><th>29</th><td>650</td><td>3.7</td><td>6</td><td>1</td></tr><tr><th>30</th><td>660</td><td>3.3</td><td>5</td><td>1</td></tr><tr><th>&vellip;</th><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td></tr></tbody></table>"
      ],
      "text/latex": [
       "\\begin{tabular}{r|cccc}\n",
       "\t& gmat & gpa & work\\_experience & admitted\\\\\n",
       "\t\\hline\n",
       "\t& Int64 & Float64 & Int64 & Int64\\\\\n",
       "\t\\hline\n",
       "\t1 & 780 & 4.0 & 3 & 1 \\\\\n",
       "\t2 & 750 & 3.9 & 4 & 1 \\\\\n",
       "\t3 & 690 & 3.3 & 3 & 0 \\\\\n",
       "\t4 & 710 & 3.7 & 5 & 1 \\\\\n",
       "\t5 & 680 & 3.9 & 4 & 0 \\\\\n",
       "\t6 & 730 & 3.7 & 6 & 1 \\\\\n",
       "\t7 & 690 & 2.3 & 1 & 0 \\\\\n",
       "\t8 & 720 & 3.3 & 4 & 1 \\\\\n",
       "\t9 & 740 & 3.3 & 5 & 1 \\\\\n",
       "\t10 & 690 & 1.7 & 1 & 0 \\\\\n",
       "\t11 & 610 & 2.7 & 3 & 0 \\\\\n",
       "\t12 & 690 & 3.7 & 5 & 1 \\\\\n",
       "\t13 & 710 & 3.7 & 6 & 1 \\\\\n",
       "\t14 & 680 & 3.3 & 4 & 0 \\\\\n",
       "\t15 & 770 & 3.3 & 3 & 1 \\\\\n",
       "\t16 & 610 & 3.0 & 1 & 0 \\\\\n",
       "\t17 & 580 & 2.7 & 4 & 0 \\\\\n",
       "\t18 & 650 & 3.7 & 6 & 1 \\\\\n",
       "\t19 & 540 & 2.7 & 2 & 0 \\\\\n",
       "\t20 & 590 & 2.3 & 3 & 0 \\\\\n",
       "\t21 & 620 & 3.3 & 2 & 1 \\\\\n",
       "\t22 & 600 & 2.0 & 1 & 0 \\\\\n",
       "\t23 & 550 & 2.3 & 4 & 0 \\\\\n",
       "\t24 & 550 & 2.7 & 1 & 0 \\\\\n",
       "\t25 & 570 & 3.0 & 2 & 0 \\\\\n",
       "\t26 & 670 & 3.3 & 6 & 1 \\\\\n",
       "\t27 & 660 & 3.7 & 4 & 1 \\\\\n",
       "\t28 & 580 & 2.3 & 2 & 0 \\\\\n",
       "\t29 & 650 & 3.7 & 6 & 1 \\\\\n",
       "\t30 & 660 & 3.3 & 5 & 1 \\\\\n",
       "\t$\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "\u001b[1m40×4 DataFrame\u001b[0m\n",
       "\u001b[1m Row \u001b[0m│\u001b[1m gmat  \u001b[0m\u001b[1m gpa     \u001b[0m\u001b[1m work_experience \u001b[0m\u001b[1m admitted \u001b[0m\n",
       "\u001b[1m     \u001b[0m│\u001b[90m Int64 \u001b[0m\u001b[90m Float64 \u001b[0m\u001b[90m Int64           \u001b[0m\u001b[90m Int64    \u001b[0m\n",
       "─────┼───────────────────────────────────────────\n",
       "   1 │   780      4.0                3         1\n",
       "   2 │   750      3.9                4         1\n",
       "   3 │   690      3.3                3         0\n",
       "   4 │   710      3.7                5         1\n",
       "   5 │   680      3.9                4         0\n",
       "   6 │   730      3.7                6         1\n",
       "   7 │   690      2.3                1         0\n",
       "   8 │   720      3.3                4         1\n",
       "   9 │   740      3.3                5         1\n",
       "  10 │   690      1.7                1         0\n",
       "  11 │   610      2.7                3         0\n",
       "  ⋮  │   ⋮       ⋮            ⋮            ⋮\n",
       "  31 │   640      3.0                1         0\n",
       "  32 │   620      2.7                2         0\n",
       "  33 │   660      4.0                4         1\n",
       "  34 │   660      3.3                6         1\n",
       "  35 │   680      3.3                5         1\n",
       "  36 │   650      2.3                1         0\n",
       "  37 │   670      2.7                2         0\n",
       "  38 │   580      3.3                1         0\n",
       "  39 │   590      1.7                4         0\n",
       "  40 │   690      3.7                5         1\n",
       "\u001b[36m                                  19 rows omitted\u001b[0m"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = CSV.read(\"candidate.csv\", DataFrame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = [[x[1], x[2], x[3]] for x in zip(data.gmat, data.gpa, data.work_experience)]\n",
    "y_data = [x for x in data.admitted];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Implementing Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. Define loss function (cross-Entropy) for given x ,y "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "L_{ce}(\\hat{y}^{i}, {y}^{i}) = -[y^{i}\\log \\hat{y}^{i} + (1-y^{i}) \\log(1 - \\hat{y}^{i})]\n",
    "$$\n",
    "\n",
    "$$Cost(w,b) = \\frac{1}{ N} \\sum_{i=1}^{N}L_{ce}(\\hat{y}^{i} , y^{i})  $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient:\n",
    "\n",
    "$$\\frac{\\partial L_{ce}(w,b;x^{i})}{\\partial w_{j}} = [\\sigma(w^{T}x^{i}+b) - y]x_{0}^{i}$$\n",
    "\n",
    "$$\\frac{\\partial L_{ce}(w,b)}{\\partial w} = \\frac{1}{ N} \\sum_{i=1}^{N}\\frac{\\partial L_{ce}(w,b;x^{i})}{\\partial w_{j}}$$\n",
    "\n",
    "$$\\frac{\\partial L_{ce}(w,b;x^{i})}{\\partial b} = [\\sigma(w^{T}x^{i}+b) - y]$$\n",
    "\n",
    "$$\\frac{\\partial L_{ce}(w,b)}{\\partial b} = \\frac{1}{ N} \\sum_{i=1}^{N}\\frac{\\partial L_{ce}(w,b;x^{i})}{\\partial b}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "average_cost (generic function with 1 method)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q(x) = 1/(1+exp(-x))\n",
    "function cross_entropy_loss(x,y,w,b)\n",
    "    return -y*log(q(w'x + b)) - (1-y)*log(1 - q(w'x+b))\n",
    "end\n",
    "function average_cost(features, labels, w, b)\n",
    "    N = length(features)\n",
    "    return (1/N)*sum([cross_entropy_loss(features[i], labels[i], w, b) for i=1:N])\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "batch_gradient_descent (generic function with 2 methods)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function batch_gradient_descent(features, labels, w, b, a )\n",
    "    del_w = [0.0 for i =1 :length(w)]\n",
    "    del_b = 0.0\n",
    "    N = length(features)\n",
    "    for i= 1:N\n",
    "        del_w += (q(w'features[i]+b)- labels[i])*features[i]\n",
    "        del_b += (q(w'features[i]+b)- labels[i])\n",
    "    end\n",
    "    \n",
    "    w = w - a*del_w\n",
    "    b = b - a*del_b\n",
    "    return w, b\n",
    "end\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the initial cost is:0.6931471805599451\n",
      "the new cost is:0.6931188566349795\n",
      "the new cost is:0.6931096471365109\n",
      "the new cost is:0.693106617309021\n"
     ]
    }
   ],
   "source": [
    "w = [0.0, 0.0]\n",
    "b = 0.0\n",
    "println(\"the initial cost is:\", average_cost(x_data, y_data, w, b))\n",
    "\n",
    "w, b = batch_gradient_descent(x_data, y_data, w, b, 0.0000001)\n",
    "println(\"the new cost is:\", average_cost(x_data, y_data, w, b))\n",
    "\n",
    "w, b = batch_gradient_descent(x_data, y_data, w, b, 0.0000001)\n",
    "println(\"the new cost is:\", average_cost(x_data, y_data, w, b))\n",
    "\n",
    "w, b = batch_gradient_descent(x_data, y_data, w, b, 0.0000001)\n",
    "println(\"the new cost is:\", average_cost(x_data, y_data, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "train_batch_gradient_descent (generic function with 1 method)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function train_batch_gradient_descent(features, labels, w, b, a, epochs)\n",
    "\n",
    "    for i = 1:epochs\n",
    "        w, b = batch_gradient_descent(features, labels, w, b, a)\n",
    "        \n",
    "        if i == 1\n",
    "            println(\"Epoch \",i, \" with loss: \", average_cost(x_data, y_data, w, b))\n",
    "        end\n",
    "        \n",
    "        if i == 100\n",
    "            println(\"Epoch \",i, \" with loss: \", average_cost(x_data, y_data, w, b))\n",
    "        end\n",
    "        \n",
    "        if i == 1000\n",
    "            println(\"Epoch \",i, \" with loss: \", average_cost(x_data, y_data, w, b))\n",
    "        end\n",
    "        \n",
    "        if i == 10000\n",
    "            println(\"Epoch \",i, \" with loss: \", average_cost(x_data, y_data, w, b))\n",
    "        end\n",
    "        \n",
    "        if i == 100000\n",
    "            println(\"Epoch \",i, \" with loss: \", average_cost(x_data, y_data, w, b))\n",
    "        end\n",
    "        \n",
    "        if i == 1000000\n",
    "            println(\"Epoch \",i, \" with loss: \", average_cost(x_data, y_data, w, b))\n",
    "        end\n",
    "        \n",
    "        if i == 10000000\n",
    "            println(\"Epoch \",i, \" with loss: \", average_cost(x_data, y_data, w, b))\n",
    "        end\n",
    "    end\n",
    "    return w, b\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 with loss: NaN\n",
      "Epoch 100 with loss: NaN\n",
      "Epoch 1000 with loss: 0.6317259643201196\n",
      "Epoch 10000 with loss: 0.6314945401000046\n",
      "Epoch 100000 with loss: 0.6292227374439416\n",
      "Epoch 1000000 with loss: 0.6100910187881761\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([-0.008173073561562461, 1.2913879051261228], 1.3534238845130555)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = randn(2)\n",
    "b = randn(1)[1]\n",
    "w, b = train_batch_gradient_descent(x_data, y_data, w, b, 0.0000001, 1000000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 with loss: 0.6100910005891025\n",
      "Epoch 100 with loss: 0.6100891989096158\n",
      "Epoch 1000 with loss: 0.6100728226213616\n",
      "Epoch 10000 with loss: 0.609909318580536\n",
      "Epoch 100000 with loss: 0.6082997505387029\n",
      "Epoch 1000000 with loss: 0.5943934057321871\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([-0.009043877966420429, 1.5107384471548835], 1.232861545977227)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "w, b = train_batch_gradient_descent(x_data, y_data, w, b, 0.0000001, 1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 with loss: 0.5943933921811396\n",
      "Epoch 100 with loss: 0.5943920506456435\n",
      "Epoch 1000 with loss: 0.5943798565214712\n",
      "Epoch 10000 with loss: 0.5942580788600162\n",
      "Epoch 100000 with loss: 0.5930564266512515\n",
      "Epoch 1000000 with loss: 0.5824467453094122\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([-0.009748110594240486, 1.6942624722896553], 1.1144420714503962)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "w, b = train_batch_gradient_descent(x_data, y_data, w, b, 0.0000001, 1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 with loss: 0.6931188566349795\n",
      "Epoch 100 with loss: 0.6930977152288289\n",
      "Epoch 1000 with loss: 0.6930282266294219\n",
      "Epoch 10000 with loss: 0.6923351299473173\n",
      "Epoch 100000 with loss: 0.6855799117618873\n",
      "Epoch 1000000 with loss: 0.6326918737673819\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([-0.0020551903863979, 0.47622113690915635], -0.11626329950708125)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "w, b = train_batch_gradient_descent(x_data, y_data, [0.0,0.0], 0.0, 0.0000001, 1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 with loss: 0.6931188566349795\n",
      "Epoch 100 with loss: 0.6930977152288289\n",
      "Epoch 1000 with loss: 0.6930282266294219\n",
      "Epoch 10000 with loss: 0.6923351299473173\n",
      "Epoch 100000 with loss: 0.6855799117618873\n",
      "Epoch 1000000 with loss: 0.6326918737673819\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([-0.0020551903863979, 0.47622113690915635], -0.11626329950708125)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "w, b = train_batch_gradient_descent(x_data, y_data, [0.0,0.0], 0.0, 0.0000001, 1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 with loss: 0.6931188566349795\n",
      "Epoch 100 with loss: 0.6930977152288289\n",
      "Epoch 1000 with loss: 0.6930282266294219\n",
      "Epoch 10000 with loss: 0.6923351299473173\n",
      "Epoch 100000 with loss: 0.6855799117618873\n",
      "Epoch 1000000 with loss: 0.6326918737673819\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([-0.0020551903863979, 0.47622113690915635], -0.11626329950708125)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "w, b = train_batch_gradient_descent(x_data, y_data, [0.0,0.0], 0.0, 0.0000001, 1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 with loss: 0.6931188566349795\n",
      "Epoch 100 with loss: 0.6930977152288289\n",
      "Epoch 1000 with loss: 0.6930282266294219\n",
      "Epoch 10000 with loss: 0.6923351299473173\n",
      "Epoch 100000 with loss: 0.6855799117618873\n",
      "Epoch 1000000 with loss: 0.6326918737673819\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([-0.0020551903863979, 0.47622113690915635], -0.11626329950708125)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "w, b = train_batch_gradient_descent(x_data, y_data, [0.0,0.0], 0.0, 0.0000001, 1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 with loss: 0.632691827205463\n",
      "Epoch 100 with loss: 0.6326872176867723\n",
      "Epoch 1000 with loss: 0.6326453230745035\n",
      "Epoch 10000 with loss: 0.6322273761510574\n",
      "Epoch 100000 with loss: 0.6281458496177578\n",
      "Epoch 1000000 with loss: 0.5954226667563611\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([-0.0036370472006028087, 0.8445273524582464], -0.22916112906791533)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "w, b = train_batch_gradient_descent(x_data, y_data, w,b, 0.0000001, 1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 with loss: 0.5954226371160484\n",
      "Epoch 100 with loss: 0.5954197027866317\n",
      "Epoch 1000 with loss: 0.5953930326541702\n",
      "Epoch 10000 with loss: 0.5951268841889991\n",
      "Epoch 100000 with loss: 0.592519647987565\n",
      "Epoch 1000000 with loss: 0.5709852249104455\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([-0.004865616495539809, 1.13654419765688], -0.33931275004419825)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "w, b = train_batch_gradient_descent(x_data, y_data, w,b, 0.0000001, 1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 with loss: 0.5709852048107705\n",
      "Epoch 100 with loss: 0.5709832149786357\n",
      "Epoch 1000 with loss: 0.5709651288345847\n",
      "Epoch 10000 with loss: 0.5707845878223877\n",
      "Epoch 100000 with loss: 0.5690106767456337\n",
      "Epoch 1000000 with loss: 0.5539523232379783\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([-0.005840241274094449, 1.3738471774117296], -0.44718732969308767)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "w, b = train_batch_gradient_descent(x_data, y_data, w,b, 0.0000001, 1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 with loss: 0.4371463917648167\n",
      "Epoch 100 with loss: 0.43714250929276643\n",
      "Epoch 1000 with loss: 0.43710724699560494\n",
      "Epoch 10000 with loss: 0.43675785292757036\n",
      "Epoch 100000 with loss: 0.4335576780510937\n",
      "Epoch 1000000 with loss: 0.4158936982271998\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([-0.007530790567283173, 0.5300604605190322, 0.9996225282048653], -0.1887423906063372)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "w, b = train_batch_gradient_descent(x_data, y_data, w,b, 0.0000001, 1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "predict (generic function with 1 method)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function predict(x, y, w, b)\n",
    "    if q(w'x + b) >= .5\n",
    "        return 1\n",
    "#         println(\"predict accepted\")\n",
    "#         y == 1 ? println(\"was accepted\") : println(\"was not accepted\")\n",
    "        \n",
    "    else\n",
    "        return 0\n",
    "#         println(\"predict not accepted\")\n",
    "#         y == 1 ? println(\"was accepted\") : println(\"was not accepted\")\n",
    "    end\n",
    "    \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2\n"
     ]
    }
   ],
   "source": [
    "# If we predict a 1 and it is a one then that's nothing\n",
    "# \n",
    "mean_error = 0.0\n",
    "for i =1:length(x_data)\n",
    "    mean_error += (predict(x_data[i], y_data[i], w, b) -y_data[i])^2\n",
    "end\n",
    "println(mean_error/length(x_data)) # The average error\n",
    "# for i =1:length(x_data)\n",
    "#     predict(x_data[i], y_data[i], w, b)\n",
    "#     println()\n",
    "# end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The average error is 0.2, we don't miss too many. For the most part with these weights and biases we are able to acurately predict whether or not the students have been accepted to the university based off their gmat, gpa, and work experience in years. So this is implementing batch gradient descent to train for logistic regression. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.5.3",
   "language": "julia",
   "name": "julia-1.5"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
