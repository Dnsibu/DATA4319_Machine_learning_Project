{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorithm: Reinforcement Learning\n",
    "## Language: Python\n",
    "## Author: Daisy Nsibu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introduction\n",
    "### 1.1. What is Reinforcement learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reinforcement learning differs from traditional supervised and unsupervised learning in a very distinct way. Reinforcement learning deals with solving [sequential decisions](https://en.wikipedia.org/wiki/Sequential_decision_making).To demonstrate this, suppose your trying to program an algorithm to play an opponent in tic-tac-toe. Under supervised learning the algorithm would take in a feature vector and map that with either regression or classification. So as you play the game on and on, any move you make early in the game may affect how well you do later on in the game. So here there is an ordered sequence of operations and standard supervised machine learning cannot deal with that. Therefore, we need to think if this game as a sequence of decisions and that's where reinforcement learning comes in.\n",
    "\n",
    "Reinforcement learning is just as old as the supervised and unsupervised machine learning technique. It was first worked on by [Bellman](https://www.informs.org/Explore/History-of-O.R.-Excellence/Biographical-Profiles/Bellman-Richard-E) in the 1950s, and this was with relation to dynamic programming. It's also closely related to [Markov decision processes](https://towardsdatascience.com/understanding-the-markov-decision-process-mdp-8f838510f150).\n",
    "Reinforcement learning was originally formulated under optimal control theory and Markov decision processes. So what's the general idea behind Reinforcement learning? \n",
    "\n",
    "### 1.2. How does Reinforcement learning work?\n",
    "\n",
    "Well, given an environment and an agent (the thing that's going to move around in the environment) acting in the environment, the agents chooses actions (one at a time). Once an agent makes an action, the environment will accept this action(s) and then it transitions to its next state. We can think of a state as an observation on the environment. So going back to our example, the environment was the tic-tac-toe board, and I am the agent who chose to put an \"o\" in the middle of the board and once I choose that action the environment is updated and the next state is a picture of the board with an \"o\" in the middle.\n",
    "\n",
    "So when an agent makes an action it changes the environment or can change the environment and that change in the environment regardless if it actually does change or not  gives you a new state. So new action new state and moreover for each action that the agent chooses to do, this new state then gives the agent a reward.  \n",
    "\n",
    "<font color=\"blue\"> $State$ </font> $\\implies$ <font color=\"green\">$Action$</font> $\\implies$ <font color=\"red\">$Reward$</font>\n",
    "    \n",
    "(<font color=\"blue\">$s_t$</font>,<font color=\"green\">$a_t$</font>, <font color=\"red\">$r_t$</font>)\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a state (s_t) of our environment and given that state $(s_t)$ the agent then makes an action $(a_t)$ and that action$(a_t)$ results in a reward $(r_t)$. This is called an *Experience*.Thus, this is the fundamental model for reinforcement learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The standard reinforcement learning framework** \n",
    "![](https://www.researchgate.net/profile/Qing-Chang-6/publication/322994112/figure/fig4/AS:631598592565318@1527596247891/The-agent-environment-interaction-in-reinforcement-learning.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![visual](https://media.arxiv-vanity.com/render-output/4745366/RL.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Visual Explained**: We are in a state <font color=\"blue\">$(s_t)$</font>. We have an agent and the agent will choose some action <font color=\"green\">($a_t$)</font> and then for that action <font color=\"green\">($a_t$)</font> the environment returns some reward <font color=\"red\">($r_t$)</font> and returns the new state <font color=\"blue\">($s_{t+1}$)</font> we'll be in.So an experience (<font color=\"blue\">$s_t$</font>,<font color=\"green\">$a_t$</font>, <font color=\"red\">$r_t$</font>) happens over one time step, and we're going to use discrete time step to describe this process. An experience, (<font color=\"blue\">$s_t$</font>,<font color=\"green\">$a_t$</font>, <font color=\"red\">$r_t$</font>), over one time step is what is known as an **episode**.\n",
    "\n",
    "An **episode** is the time horizon from $t=0$ to either a terminal state where we finish the processes, or a maximum allowed time step. \n",
    "*Note: these processes can happen indefinitely.*\n",
    "\n",
    "A sequence of experiences over an episode is what's known as a **tracjectory**, $$\\tau = (s_0,a_0,r_0), \\dotsb,(s_T,a_T,r_T) $$\n",
    "\n",
    "*Note: For this notebook I am  going to use \"T\" to denote the final time step in a episode always. So \"T\" is either the maximum time step or the time step when we exit out of an episode.*\n",
    "\n",
    "The agent looks at the state, makes an action, and there's a reward. It does it again and again all the way to the end where there's the last state $(s_T)$, the last action $(a_T)$ and  the last reward $(r_T)$. This is how the agent is playing the game."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you may be wondering, but how does an agent choose the action? So let's come up with a term for how an agent chooses an action, and this is what's referred to as the **agent's policy**.\n",
    "The policy of the agent is what determines the action that it will make. The policy will determine the action given the state, so this is a function. The inputs would be the states and the output would be the action in that environment.\n",
    "\n",
    "For a given trajectory, which is a sequence of experiences over a given episode, we want to maximize the rewards of a given trajectory.\n",
    "Thus, the goal would be to maximize the agent's rewards.\n",
    "The general goal is to get the agents to do better and better each play, and in order to achieve this we optimize our rewards."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Understanding the Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Notation We Need:\n",
    "time $t$ in a given environment\n",
    "\n",
    "*  $s_t$ $\\implies S$ is the State, $S$  is the set of all possible states, State space\n",
    "*  $a_t$ $\\implies A$  is the action,  $A$  is the set of all possible actions, action space\n",
    "*  $r_t =  \\mathcal{R} (s_t, a_t, s_{t+1})$ is the reward, $\\mathcal{R}$, this is called the reward function\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the agent makes a decision to take an action, the state space transitions to a new state, possibly the equivalent state but it still transitions to a new state at a different time. So that's done by way of what's called ,the **transition function**. Environments transition from one state to another state to the next by using a transition function. This transition function function is going to be formulated  by way of a Markov decision process (MDP). MPD gives the next state which is , $(s_{t+1}) \\backsim P(s_{t+1} | s_{t}, a_{t})$, so this is called the markov property.\n",
    "\n",
    "For a given environment the **mdp formulation**, markov decision\n",
    "property formulation, or reinforcementlearning requires we have a state space, $s$ , and action space $a$, a transition function $P(s_{t+1} | s_{t}, a_{t})$, and a reward function $\\mathcal{R} $. If we have all of these things here, then we can formally define an optimization problem and an algorithm that would help us solve this. Our goal is to somehow\n",
    "maximize the rewards for our agent ,who is making decisions or actions in that environment, where the rewards are taking over a\n",
    "trajectory of experiences which are these triples, $(s_t, a_t, r_{t})$, of state action rewards.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Objective:\n",
    "$$\\tau = (s_0,a_0,r_0), \\dotsb,(s_T,a_T,r_T) $$ is a trajectory for a given episode. An episode is just a sequence of time steps from time is equal to zero to a terminal state or through a maximum allowed time. The way that we're going to measure our reward is with what's called a\n",
    "return and the return is basically a weighted reward.\n",
    "The scaling or a weighting of the rewards for each triple, $(s_T,a_T,r_T) $\n",
    "\n",
    "Return:\n",
    "${R}(\\tau) = \\sum\\limits_{i=0}^{T}{\\gamma^t}r_t$, $t \\in [0,1]$\n",
    "For a given trajectory this is the return of that trajectory it's a\n",
    "weighted sum of each of those rewards.\n",
    "\n",
    " \n",
    "**Objective function:**\n",
    "${J}(\\tau) = \\underset{\\tau \\backsim \\pi}{\\mathbb{E}}[{R}(\\tau)]$\n",
    "We want, in the end, to maximize this expectation, the average rewards for a given trajectory.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Pseudocode for the General MDP Control Loop.\n",
    "\n",
    "*The main thing we're missing is the optimization part, where we want to maximize the objective*\n",
    "\n",
    "1. Given an environment(env) and an agent\n",
    "\n",
    "2. for episode = 0,..., MAX_EPISODE do\n",
    "\n",
    "3. state = env.reset()\n",
    "\n",
    "4. agent.reset()\n",
    "\n",
    "5. for t=0, ..., T do\n",
    "\n",
    "6. action = agent.act(state)\n",
    "\n",
    "7. state, reward = env.step(action)\n",
    "\n",
    "8. agent.update(action, state, reward)\n",
    "\n",
    "9. if env.done()\n",
    "\n",
    "10. break\n",
    "\n",
    "11. end if\n",
    "\n",
    "12. end for\n",
    "\n",
    "13. end for\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4. What Should the Agent Learn?\n",
    "There are two main functions that an agent can learn in reinforcement learning:\n",
    "\n",
    "\n",
    "1. A **Policy**, $\\pi$ , which maps state to action: $a \\backsim {\\pi(s)}$.\n",
    "\n",
    "    * Policy methods learn a parameterized policy which produces action probabilities from states.\n",
    "2. A **Value function** and there two main ways to assess value;\n",
    "    *  $V^{\\pi}(s) = \\underset{s_t= s,  \\tau \\backsim \\pi}{\\mathbb{E}} [{\\sum\\limits_{i=0}^{T} }{\\gamma^t}r_t] \\implies$ how good or bad a current state is on avergae\n",
    "    \n",
    "    * $ Q^{\\pi}(s,a) =  \\underset{s_t= s, a_t=a, \\tau \\backsim \\pi}{\\mathbb{E}} [{\\sum\\limits_{i=0}^{T} }{\\gamma^t}r_t] \\implies$The expected return of taking action *a* in state *s*\n",
    "    \n",
    "    * Value based methods learn the value of actions and then select actions based off of their expected returns.\n",
    "\n",
    "3. The **transition function**, $P(S_{t+1} | a_{t}, s_{t}) \\approx$ \"Imagining the future\"\n",
    "    \n",
    "For our purposes in the notebook, I'll discuss  about a\n",
    "policy-based method called the policy gradient, in particular called **reinforce algoritm**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. The Reinforce Algorithm (Deep Reinforcement learning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **Reinforce algorithm** is a policy based method, that is also a deep reinforcing algorithm. This algorithm was invented in 1992, due to [Ronald Williams](https://en.wikipedia.org/wiki/Ronald_J._Williams). Since it's a policy based method it learns a parameterized policy.\n",
    "\n",
    "**Key Idea behind this algorithm:**\n",
    "During learning, actions that resulted in desire your good outcomes\n",
    "should become more probable. So this is where the stochastic nature\n",
    "the non-deterministic nature of policy methods is really emphasized.\n",
    "So in other words these actions are positively reinforced. Conversely,  actions that resulted in bad outcomes should become less probable.\n",
    "\n",
    "A policy $\\pi$ is a function, mapping states to probabilities and these probabilities are used to sample an action, $a \\backsim \\pi(s)$\n",
    "\n",
    "I am going to represent this policy with the neural networks. So\n",
    "suppose our input space consists of four things . We can feed those in to a network, a fully connected multi-layer perceptron and of course there are a bunch of weights.Let's say that each of these\n",
    "weights is going to be some $\\theta$,  so $\\theta$ are the weights\n",
    "of this network.\n",
    "\n",
    "In this notebook, I'll be using the CartPole environment. [*CartPole*](https://gym.openai.com/envs/CartPole-v1/) is a traditional reinforcement learning task where you have to balance the pole on a cart. \n",
    " \n",
    "So the action space is either left or right, and the input space is velocity, angle of the pole, position, and  angular velocity of the pole. So if we have two possible actions we can have our output of this neural network look like this below.\n",
    "\n",
    "![Policy Network](https://www.researchgate.net/profile/Hany-Elsalamony/publication/263054095/figure/fig1/AS:296595871551488@1447725374077/The-structure-of-multilayer-perceptron-neural-network-Weights-w-ij-in-Equation-1-are.png)\n",
    "\n",
    "So the one output would be, $\\pi_{\\theta}(a_1|s_t)$ and the other would be, $\\pi_{\\theta}(a_2|s_t)$ and then we sample from that probability\n",
    "distribution one of those actions. It's not guaranteed that we take the maximum we sample.This is because it's\n",
    "**not deterministic**, we're sampling from this distribution\n",
    "which is induced by the weights in this network. \n",
    "\n",
    "The weights in this network do some distribution 2) sample 3) make an action 4) get a reward 5) have a new state 6) feed that state in 7) have some distribution 8)\n",
    "sample an action from that distribution 9) get a reward for updating according\n",
    "to that action and repeat. The choice of weights in here is going to produce many different trajectories. $\\tau = (s_0,a_0,r_0), \\dotsb,(s_T,a_T,r_T)$. Now the trajectories are going to differ\n",
    "because we're sampling from a distribution here we're not guaranteed\n",
    "to always have the same trajectory for a given set of weights in\n",
    "this policy network. so what we want to do is we want to maximize the expected the average return over this over a given policy network.\n",
    "\n",
    "So that's precisely what we want to do with the policy gradients. We want to maximize, according to the parameters\n",
    "$\\theta$ which are the weights of this policy network, the expected return so where tau is\n",
    "sampled from that distribution according weights theta of the returns.\n",
    "$$ \\underset{\\theta}{max} \\underset{\\tau \\backsim {\\pi_{\\theta}} }{\\mathbb{E}} [R(\\tau)] \\implies J(\\pi_\\theta)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reinforce\n",
    "So in multi-layer perceptron you've probably seen gradient descent which minimizes your loss right you're minimizing a function well if you want to maximize something you use gradient ascent instead of going in the downhill direction you go in the uphill direction. The way that we can do this is with gradient ascent. $ \\theta \\Longleftarrow \\theta + \\alpha * \\nabla_{\\theta}J(\\pi_\\theta)\n",
    "$ alpha is the learning rate. \n",
    "\n",
    "The gradient is given by the following formula: \n",
    "$$ \\nabla_{\\theta}J(\\pi_\\theta) = \\underset{\\tau \\backsim {\\pi_{\\theta}} }{\\mathbb{E}} [{\\sum\\limits_{t=0}^{T}}R(\\tau)\\nabla_{\\theta}log(a_t|s_t)] = \\underset{\\tau \\backsim {\\pi_{\\theta}}}{\\mathbb{E}} [{\\sum\\limits_{t=0}^{T}}R_{t}(\\tau)\\nabla_{\\theta}log \\pi_{\\theta}(a_t|s_t)]\n",
    "$$\n",
    "Because of [Monte Carlo Sampling](https://en.wikipedia.org/wiki/Monte_Carlo_method)\n",
    "\n",
    "$R_{t}(\\tau) = {\\sum\\limits_{t'=0}^{T}}\\gamma^{t'-t}r_{t} $ is the return over the trajectory starting at time t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Pseudocode for the Reinforce Algorithm \n",
    "\n",
    "\n",
    "1. choose learnign rate, $\\alpha$\n",
    "\n",
    "2. Initialize weights $\\theta$ for policy network\n",
    "\n",
    "3. for episode = 0,..., MAX_EPISODE do\n",
    "\n",
    "4. sample a trajectory $\\tau$\n",
    "\n",
    "5. set $\\nabla_{\\theta}J(\\pi_{\\theta}) = 0$\n",
    "\n",
    "6. for *t* = 0, ..., T do\n",
    "\n",
    "7. $R_{t}(\\tau)= {\\sum\\limits_{t'=0}^{T}}\\gamma^{t'-t}r_{t'}$\n",
    "\n",
    "8. $\\nabla_{\\theta}J(\\pi_\\theta)= \\nabla_{\\theta}J(\\pi_{\\theta}) + R_{t}(\\tau)\\nabla_{\\theta}log{\\pi_{\\theta}}(a_t|s_t)$\n",
    "\n",
    "9. end for\n",
    "\n",
    "10. $\\theta = \\theta + \\alpha * \\nabla_{\\theta}J(\\pi_\\theta)$\n",
    "\n",
    "11. end for"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Implementing the Reinforce Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch \n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# pip install gym\n",
    "import gym\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNetwork(nn.Module):\n",
    "    \n",
    "    \n",
    "    def __init__(self, in_dim, hidden_layer_size, out_dim):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        layers = [nn.Linear(in_dim, hidden_layer_size),\n",
    "                 nn.ReLU(),  \n",
    "                 nn.Linear(hidden_layer_size, out_dim),]\n",
    "        self.model = nn.Sequential(*layers)\n",
    "        self.onpolicy_reset()\n",
    "        self.train()\n",
    "     \n",
    "    \n",
    "    def onpolicy_reset(self):\n",
    "        self.log_probs = []\n",
    "        self.rewards = []\n",
    "        \n",
    "        \n",
    "    def forward_pass(self, x):\n",
    "        pdparams = self.model(x)\n",
    "        return pdparams\n",
    "    \n",
    "    \n",
    "    def act(self, state):\n",
    "        x = torch.from_numpy(state.astype(np.float32))\n",
    "        pdparams = self.forward_pass(x)\n",
    "        pd = torch.distributions.Categorical(logits = pdparams)\n",
    "        action = pd.sample() # pi(a|s)\n",
    "        log_prob = pd.log_prob(action) # log[ pi(a|s) ]\n",
    "        self.log_probs.append(log_prob)\n",
    "        return action.item()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(pi, optimizer, gamma):\n",
    "    \n",
    "    T = len(pi.rewards)\n",
    "    trajectory_returns = np.empty(T, dtype = np.float32)\n",
    "    future_returns = 0.0\n",
    "    \n",
    "    \n",
    "    for t in reversed(range(T)):\n",
    "        future_returns = pi.rewards[t] + gamma*future_returns\n",
    "        trajectory_returns[t] = future_returns\n",
    "\n",
    "    trajectory_returns = torch.tensor(trajectory_returns)\n",
    "    log_probs = torch.stack(pi.log_probs)\n",
    "    loss = - log_probs*trajectory_returns\n",
    "    loss = torch.sum(loss)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()     # Back propogate \n",
    "    optimizer.step()    # gradient ascent and update the weights (all built into torch)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(baseline = False, gamma = 0.99, swing = False):\n",
    "    env = gym.make('CartPole-v0')\n",
    "    in_dim = env.observation_space.shape[0]\n",
    "    out_dim = env.action_space.n\n",
    "    pi = PolicyNetwork(in_dim, 64, out_dim)\n",
    "    optimizer = optim.Adam(pi.parameters(), lr = 0.01)\n",
    "    \n",
    "    for epi in range(300):\n",
    "        state = env.reset()\n",
    "        for t in range(200):\n",
    "            action = pi.act(state)\n",
    "            state, reward, done, _ = env.step(action)\n",
    "            pi.rewards.append(reward)\n",
    "            env.render()\n",
    "            #if done:\n",
    "            if swing == True:\n",
    "                if state[0] < -4.7 or state[0] > 4.7:\n",
    "                    break\n",
    "            else:\n",
    "                if done:\n",
    "                    break\n",
    "        if baseline == True:\n",
    "            b = (1/len(pi.rewards))*sum(pi.rewards)\n",
    "            pi.rewards = [(r - b) for r in pi.rewards]\n",
    "            \n",
    "        loss = train(pi, optimizer, gamma)\n",
    "        total_reward = sum(pi.rewards)\n",
    "        solved = total_reward > 195.0\n",
    "        pi.onpolicy_reset()\n",
    "        print(f'Episode: {epi}')\n",
    "        print(f'Total Reward: {total_reward}')\n",
    "        print(f'Solved: {solved}')\n",
    "        print('')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0\n",
      "Total Reward: 18.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 1\n",
      "Total Reward: 20.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 2\n",
      "Total Reward: 17.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 3\n",
      "Total Reward: 17.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 4\n",
      "Total Reward: 12.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 5\n",
      "Total Reward: 28.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 6\n",
      "Total Reward: 16.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 7\n",
      "Total Reward: 11.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 8\n",
      "Total Reward: 33.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 9\n",
      "Total Reward: 20.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 10\n",
      "Total Reward: 16.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 11\n",
      "Total Reward: 22.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 12\n",
      "Total Reward: 33.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 13\n",
      "Total Reward: 32.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 14\n",
      "Total Reward: 52.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 15\n",
      "Total Reward: 48.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 16\n",
      "Total Reward: 16.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 17\n",
      "Total Reward: 10.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 18\n",
      "Total Reward: 62.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 19\n",
      "Total Reward: 22.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 20\n",
      "Total Reward: 27.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 21\n",
      "Total Reward: 13.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 22\n",
      "Total Reward: 40.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 23\n",
      "Total Reward: 26.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 24\n",
      "Total Reward: 33.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 25\n",
      "Total Reward: 59.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 26\n",
      "Total Reward: 92.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 27\n",
      "Total Reward: 88.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 28\n",
      "Total Reward: 31.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 29\n",
      "Total Reward: 22.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 30\n",
      "Total Reward: 36.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 31\n",
      "Total Reward: 25.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 32\n",
      "Total Reward: 79.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 33\n",
      "Total Reward: 40.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 34\n",
      "Total Reward: 110.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 35\n",
      "Total Reward: 63.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 36\n",
      "Total Reward: 84.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 37\n",
      "Total Reward: 107.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 38\n",
      "Total Reward: 58.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 39\n",
      "Total Reward: 65.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 40\n",
      "Total Reward: 65.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 41\n",
      "Total Reward: 67.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 42\n",
      "Total Reward: 22.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 43\n",
      "Total Reward: 44.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 44\n",
      "Total Reward: 86.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 45\n",
      "Total Reward: 120.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 46\n",
      "Total Reward: 103.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 47\n",
      "Total Reward: 67.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 48\n",
      "Total Reward: 101.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 49\n",
      "Total Reward: 57.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 50\n",
      "Total Reward: 66.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 51\n",
      "Total Reward: 58.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 52\n",
      "Total Reward: 35.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 53\n",
      "Total Reward: 37.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 54\n",
      "Total Reward: 25.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 55\n",
      "Total Reward: 31.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 56\n",
      "Total Reward: 25.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 57\n",
      "Total Reward: 33.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 58\n",
      "Total Reward: 39.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 59\n",
      "Total Reward: 24.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 60\n",
      "Total Reward: 40.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 61\n",
      "Total Reward: 43.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 62\n",
      "Total Reward: 41.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 63\n",
      "Total Reward: 22.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 64\n",
      "Total Reward: 25.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 65\n",
      "Total Reward: 33.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 66\n",
      "Total Reward: 37.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 67\n",
      "Total Reward: 40.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 68\n",
      "Total Reward: 42.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 69\n",
      "Total Reward: 33.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 70\n",
      "Total Reward: 37.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 71\n",
      "Total Reward: 65.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 72\n",
      "Total Reward: 26.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 73\n",
      "Total Reward: 32.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 74\n",
      "Total Reward: 45.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 75\n",
      "Total Reward: 69.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 76\n",
      "Total Reward: 57.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 77\n",
      "Total Reward: 40.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 78\n",
      "Total Reward: 52.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 79\n",
      "Total Reward: 59.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 80\n",
      "Total Reward: 83.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 81\n",
      "Total Reward: 48.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 82\n",
      "Total Reward: 49.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 83\n",
      "Total Reward: 101.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 84\n",
      "Total Reward: 144.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 85\n",
      "Total Reward: 123.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 86\n",
      "Total Reward: 109.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 87\n",
      "Total Reward: 200.0\n",
      "Solved: True\n",
      "\n",
      "Episode: 88\n",
      "Total Reward: 120.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 89\n",
      "Total Reward: 120.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 90\n",
      "Total Reward: 200.0\n",
      "Solved: True\n",
      "\n",
      "Episode: 91\n",
      "Total Reward: 160.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 92\n",
      "Total Reward: 190.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 93\n",
      "Total Reward: 190.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 94\n",
      "Total Reward: 200.0\n",
      "Solved: True\n",
      "\n",
      "Episode: 95\n",
      "Total Reward: 108.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 96\n",
      "Total Reward: 165.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 97\n",
      "Total Reward: 123.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 98\n",
      "Total Reward: 155.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 99\n",
      "Total Reward: 93.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 100\n",
      "Total Reward: 200.0\n",
      "Solved: True\n",
      "\n",
      "Episode: 101\n",
      "Total Reward: 199.0\n",
      "Solved: True\n",
      "\n",
      "Episode: 102\n",
      "Total Reward: 200.0\n",
      "Solved: True\n",
      "\n",
      "Episode: 103\n",
      "Total Reward: 200.0\n",
      "Solved: True\n",
      "\n",
      "Episode: 104\n",
      "Total Reward: 200.0\n",
      "Solved: True\n",
      "\n",
      "Episode: 105\n",
      "Total Reward: 200.0\n",
      "Solved: True\n",
      "\n",
      "Episode: 106\n",
      "Total Reward: 200.0\n",
      "Solved: True\n",
      "\n",
      "Episode: 107\n",
      "Total Reward: 200.0\n",
      "Solved: True\n",
      "\n",
      "Episode: 108\n",
      "Total Reward: 200.0\n",
      "Solved: True\n",
      "\n",
      "Episode: 109\n",
      "Total Reward: 200.0\n",
      "Solved: True\n",
      "\n",
      "Episode: 110\n",
      "Total Reward: 200.0\n",
      "Solved: True\n",
      "\n",
      "Episode: 111\n",
      "Total Reward: 200.0\n",
      "Solved: True\n",
      "\n",
      "Episode: 112\n",
      "Total Reward: 200.0\n",
      "Solved: True\n",
      "\n",
      "Episode: 113\n",
      "Total Reward: 183.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 114\n",
      "Total Reward: 149.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 115\n",
      "Total Reward: 157.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 116\n",
      "Total Reward: 138.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 117\n",
      "Total Reward: 18.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 118\n",
      "Total Reward: 107.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 119\n",
      "Total Reward: 98.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 120\n",
      "Total Reward: 17.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 121\n",
      "Total Reward: 129.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 122\n",
      "Total Reward: 111.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 123\n",
      "Total Reward: 153.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 124\n",
      "Total Reward: 141.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 125\n",
      "Total Reward: 157.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 126\n",
      "Total Reward: 173.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 127\n",
      "Total Reward: 162.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 128\n",
      "Total Reward: 174.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 129\n",
      "Total Reward: 181.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 130\n",
      "Total Reward: 200.0\n",
      "Solved: True\n",
      "\n",
      "Episode: 131\n",
      "Total Reward: 200.0\n",
      "Solved: True\n",
      "\n",
      "Episode: 132\n",
      "Total Reward: 200.0\n",
      "Solved: True\n",
      "\n",
      "Episode: 133\n",
      "Total Reward: 200.0\n",
      "Solved: True\n",
      "\n",
      "Episode: 134\n",
      "Total Reward: 200.0\n",
      "Solved: True\n",
      "\n",
      "Episode: 135\n",
      "Total Reward: 200.0\n",
      "Solved: True\n",
      "\n",
      "Episode: 136\n",
      "Total Reward: 200.0\n",
      "Solved: True\n",
      "\n",
      "Episode: 137\n",
      "Total Reward: 200.0\n",
      "Solved: True\n",
      "\n",
      "Episode: 138\n",
      "Total Reward: 200.0\n",
      "Solved: True\n",
      "\n",
      "Episode: 139\n",
      "Total Reward: 200.0\n",
      "Solved: True\n",
      "\n",
      "Episode: 140\n",
      "Total Reward: 200.0\n",
      "Solved: True\n",
      "\n",
      "Episode: 141\n",
      "Total Reward: 200.0\n",
      "Solved: True\n",
      "\n",
      "Episode: 142\n",
      "Total Reward: 200.0\n",
      "Solved: True\n",
      "\n",
      "Episode: 143\n",
      "Total Reward: 200.0\n",
      "Solved: True\n",
      "\n",
      "Episode: 144\n",
      "Total Reward: 200.0\n",
      "Solved: True\n",
      "\n",
      "Episode: 145\n",
      "Total Reward: 200.0\n",
      "Solved: True\n",
      "\n",
      "Episode: 146\n",
      "Total Reward: 200.0\n",
      "Solved: True\n",
      "\n",
      "Episode: 147\n",
      "Total Reward: 200.0\n",
      "Solved: True\n",
      "\n",
      "Episode: 148\n",
      "Total Reward: 200.0\n",
      "Solved: True\n",
      "\n",
      "Episode: 149\n",
      "Total Reward: 200.0\n",
      "Solved: True\n",
      "\n",
      "Episode: 150\n",
      "Total Reward: 200.0\n",
      "Solved: True\n",
      "\n",
      "Episode: 151\n",
      "Total Reward: 200.0\n",
      "Solved: True\n",
      "\n",
      "Episode: 152\n",
      "Total Reward: 200.0\n",
      "Solved: True\n",
      "\n",
      "Episode: 153\n",
      "Total Reward: 200.0\n",
      "Solved: True\n",
      "\n",
      "Episode: 154\n",
      "Total Reward: 200.0\n",
      "Solved: True\n",
      "\n",
      "Episode: 155\n",
      "Total Reward: 200.0\n",
      "Solved: True\n",
      "\n",
      "Episode: 156\n",
      "Total Reward: 200.0\n",
      "Solved: True\n",
      "\n",
      "Episode: 157\n",
      "Total Reward: 200.0\n",
      "Solved: True\n",
      "\n",
      "Episode: 158\n",
      "Total Reward: 200.0\n",
      "Solved: True\n",
      "\n",
      "Episode: 159\n",
      "Total Reward: 200.0\n",
      "Solved: True\n",
      "\n",
      "Episode: 160\n",
      "Total Reward: 200.0\n",
      "Solved: True\n",
      "\n",
      "Episode: 161\n",
      "Total Reward: 200.0\n",
      "Solved: True\n",
      "\n",
      "Episode: 162\n",
      "Total Reward: 200.0\n",
      "Solved: True\n",
      "\n",
      "Episode: 163\n",
      "Total Reward: 200.0\n",
      "Solved: True\n",
      "\n",
      "Episode: 164\n",
      "Total Reward: 200.0\n",
      "Solved: True\n",
      "\n",
      "Episode: 165\n",
      "Total Reward: 200.0\n",
      "Solved: True\n",
      "\n",
      "Episode: 166\n",
      "Total Reward: 200.0\n",
      "Solved: True\n",
      "\n",
      "Episode: 167\n",
      "Total Reward: 200.0\n",
      "Solved: True\n",
      "\n",
      "Episode: 168\n",
      "Total Reward: 200.0\n",
      "Solved: True\n",
      "\n",
      "Episode: 169\n",
      "Total Reward: 200.0\n",
      "Solved: True\n",
      "\n",
      "Episode: 170\n",
      "Total Reward: 200.0\n",
      "Solved: True\n",
      "\n",
      "Episode: 171\n",
      "Total Reward: 194.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 172\n",
      "Total Reward: 200.0\n",
      "Solved: True\n",
      "\n",
      "Episode: 173\n",
      "Total Reward: 200.0\n",
      "Solved: True\n",
      "\n",
      "Episode: 174\n",
      "Total Reward: 200.0\n",
      "Solved: True\n",
      "\n",
      "Episode: 175\n",
      "Total Reward: 200.0\n",
      "Solved: True\n",
      "\n",
      "Episode: 176\n",
      "Total Reward: 200.0\n",
      "Solved: True\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 177\n",
      "Total Reward: 112.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 178\n",
      "Total Reward: 98.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 179\n",
      "Total Reward: 194.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 180\n",
      "Total Reward: 200.0\n",
      "Solved: True\n",
      "\n",
      "Episode: 181\n",
      "Total Reward: 197.0\n",
      "Solved: True\n",
      "\n",
      "Episode: 182\n",
      "Total Reward: 105.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 183\n",
      "Total Reward: 200.0\n",
      "Solved: True\n",
      "\n",
      "Episode: 184\n",
      "Total Reward: 89.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 185\n",
      "Total Reward: 112.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 186\n",
      "Total Reward: 175.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 187\n",
      "Total Reward: 200.0\n",
      "Solved: True\n",
      "\n",
      "Episode: 188\n",
      "Total Reward: 172.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 189\n",
      "Total Reward: 200.0\n",
      "Solved: True\n",
      "\n",
      "Episode: 190\n",
      "Total Reward: 200.0\n",
      "Solved: True\n",
      "\n",
      "Episode: 191\n",
      "Total Reward: 94.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 192\n",
      "Total Reward: 146.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 193\n",
      "Total Reward: 113.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 194\n",
      "Total Reward: 108.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 195\n",
      "Total Reward: 113.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 196\n",
      "Total Reward: 109.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 197\n",
      "Total Reward: 51.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 198\n",
      "Total Reward: 117.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 199\n",
      "Total Reward: 25.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 200\n",
      "Total Reward: 55.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 201\n",
      "Total Reward: 41.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 202\n",
      "Total Reward: 40.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 203\n",
      "Total Reward: 43.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 204\n",
      "Total Reward: 57.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 205\n",
      "Total Reward: 39.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 206\n",
      "Total Reward: 40.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 207\n",
      "Total Reward: 61.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 208\n",
      "Total Reward: 33.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 209\n",
      "Total Reward: 56.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 210\n",
      "Total Reward: 62.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 211\n",
      "Total Reward: 32.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 212\n",
      "Total Reward: 45.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 213\n",
      "Total Reward: 49.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 214\n",
      "Total Reward: 53.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 215\n",
      "Total Reward: 25.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 216\n",
      "Total Reward: 44.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 217\n",
      "Total Reward: 72.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 218\n",
      "Total Reward: 50.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 219\n",
      "Total Reward: 41.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 220\n",
      "Total Reward: 58.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 221\n",
      "Total Reward: 74.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 222\n",
      "Total Reward: 65.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 223\n",
      "Total Reward: 96.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 224\n",
      "Total Reward: 46.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 225\n",
      "Total Reward: 38.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 226\n",
      "Total Reward: 53.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 227\n",
      "Total Reward: 115.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 228\n",
      "Total Reward: 103.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 229\n",
      "Total Reward: 112.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 230\n",
      "Total Reward: 112.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 231\n",
      "Total Reward: 57.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 232\n",
      "Total Reward: 110.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 233\n",
      "Total Reward: 118.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 234\n",
      "Total Reward: 117.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 235\n",
      "Total Reward: 127.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 236\n",
      "Total Reward: 121.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 237\n",
      "Total Reward: 115.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 238\n",
      "Total Reward: 105.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 239\n",
      "Total Reward: 121.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 240\n",
      "Total Reward: 122.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 241\n",
      "Total Reward: 127.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 242\n",
      "Total Reward: 137.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 243\n",
      "Total Reward: 123.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 244\n",
      "Total Reward: 147.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 245\n",
      "Total Reward: 137.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 246\n",
      "Total Reward: 121.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 247\n",
      "Total Reward: 143.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 248\n",
      "Total Reward: 132.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 249\n",
      "Total Reward: 135.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 250\n",
      "Total Reward: 140.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 251\n",
      "Total Reward: 121.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 252\n",
      "Total Reward: 149.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 253\n",
      "Total Reward: 130.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 254\n",
      "Total Reward: 116.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 255\n",
      "Total Reward: 120.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 256\n",
      "Total Reward: 119.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 257\n",
      "Total Reward: 146.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 258\n",
      "Total Reward: 113.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 259\n",
      "Total Reward: 129.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 260\n",
      "Total Reward: 158.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 261\n",
      "Total Reward: 153.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 262\n",
      "Total Reward: 141.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 263\n",
      "Total Reward: 136.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 264\n",
      "Total Reward: 178.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 265\n",
      "Total Reward: 146.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 266\n",
      "Total Reward: 170.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 267\n",
      "Total Reward: 200.0\n",
      "Solved: True\n",
      "\n",
      "Episode: 268\n",
      "Total Reward: 188.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 269\n",
      "Total Reward: 119.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 270\n",
      "Total Reward: 200.0\n",
      "Solved: True\n",
      "\n",
      "Episode: 271\n",
      "Total Reward: 149.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 272\n",
      "Total Reward: 200.0\n",
      "Solved: True\n",
      "\n",
      "Episode: 273\n",
      "Total Reward: 200.0\n",
      "Solved: True\n",
      "\n",
      "Episode: 274\n",
      "Total Reward: 200.0\n",
      "Solved: True\n",
      "\n",
      "Episode: 275\n",
      "Total Reward: 200.0\n",
      "Solved: True\n",
      "\n",
      "Episode: 276\n",
      "Total Reward: 200.0\n",
      "Solved: True\n",
      "\n",
      "Episode: 277\n",
      "Total Reward: 200.0\n",
      "Solved: True\n",
      "\n",
      "Episode: 278\n",
      "Total Reward: 185.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 279\n",
      "Total Reward: 200.0\n",
      "Solved: True\n",
      "\n",
      "Episode: 280\n",
      "Total Reward: 200.0\n",
      "Solved: True\n",
      "\n",
      "Episode: 281\n",
      "Total Reward: 127.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 282\n",
      "Total Reward: 200.0\n",
      "Solved: True\n",
      "\n",
      "Episode: 283\n",
      "Total Reward: 200.0\n",
      "Solved: True\n",
      "\n",
      "Episode: 284\n",
      "Total Reward: 132.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 285\n",
      "Total Reward: 200.0\n",
      "Solved: True\n",
      "\n",
      "Episode: 286\n",
      "Total Reward: 200.0\n",
      "Solved: True\n",
      "\n",
      "Episode: 287\n",
      "Total Reward: 120.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 288\n",
      "Total Reward: 156.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 289\n",
      "Total Reward: 150.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 290\n",
      "Total Reward: 143.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 291\n",
      "Total Reward: 152.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 292\n",
      "Total Reward: 151.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 293\n",
      "Total Reward: 117.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 294\n",
      "Total Reward: 160.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 295\n",
      "Total Reward: 200.0\n",
      "Solved: True\n",
      "\n",
      "Episode: 296\n",
      "Total Reward: 200.0\n",
      "Solved: True\n",
      "\n",
      "Episode: 297\n",
      "Total Reward: 200.0\n",
      "Solved: True\n",
      "\n",
      "Episode: 298\n",
      "Total Reward: 200.0\n",
      "Solved: True\n",
      "\n",
      "Episode: 299\n",
      "Total Reward: 200.0\n",
      "Solved: True\n",
      "\n"
     ]
    }
   ],
   "source": [
    "main(gamma=0.99)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " I consider a  different scaling factor to see how it behaved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0\n",
      "Total Reward: 28.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 1\n",
      "Total Reward: 22.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 2\n",
      "Total Reward: 55.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 3\n",
      "Total Reward: 23.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 4\n",
      "Total Reward: 29.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 5\n",
      "Total Reward: 11.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 6\n",
      "Total Reward: 20.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 7\n",
      "Total Reward: 25.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 8\n",
      "Total Reward: 40.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 9\n",
      "Total Reward: 26.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 10\n",
      "Total Reward: 15.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 11\n",
      "Total Reward: 38.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 12\n",
      "Total Reward: 15.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 13\n",
      "Total Reward: 36.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 14\n",
      "Total Reward: 37.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 15\n",
      "Total Reward: 29.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 16\n",
      "Total Reward: 33.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 17\n",
      "Total Reward: 71.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 18\n",
      "Total Reward: 29.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 19\n",
      "Total Reward: 64.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 20\n",
      "Total Reward: 29.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 21\n",
      "Total Reward: 63.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 22\n",
      "Total Reward: 33.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 23\n",
      "Total Reward: 47.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 24\n",
      "Total Reward: 84.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 25\n",
      "Total Reward: 27.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 26\n",
      "Total Reward: 40.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 27\n",
      "Total Reward: 45.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 28\n",
      "Total Reward: 126.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 29\n",
      "Total Reward: 41.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 30\n",
      "Total Reward: 73.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 31\n",
      "Total Reward: 139.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 32\n",
      "Total Reward: 89.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 33\n",
      "Total Reward: 93.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 34\n",
      "Total Reward: 146.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 35\n",
      "Total Reward: 92.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 36\n",
      "Total Reward: 66.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 37\n",
      "Total Reward: 19.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 38\n",
      "Total Reward: 191.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 39\n",
      "Total Reward: 108.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 40\n",
      "Total Reward: 56.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 41\n",
      "Total Reward: 40.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 42\n",
      "Total Reward: 72.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 43\n",
      "Total Reward: 164.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 44\n",
      "Total Reward: 63.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 45\n",
      "Total Reward: 43.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 46\n",
      "Total Reward: 63.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 47\n",
      "Total Reward: 200.0\n",
      "Solved: True\n",
      "\n",
      "Episode: 48\n",
      "Total Reward: 41.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 49\n",
      "Total Reward: 137.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 50\n",
      "Total Reward: 37.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 51\n",
      "Total Reward: 60.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 52\n",
      "Total Reward: 80.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 53\n",
      "Total Reward: 127.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 54\n",
      "Total Reward: 75.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 55\n",
      "Total Reward: 106.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 56\n",
      "Total Reward: 82.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 57\n",
      "Total Reward: 74.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 58\n",
      "Total Reward: 103.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 59\n",
      "Total Reward: 85.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 60\n",
      "Total Reward: 72.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 61\n",
      "Total Reward: 94.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 62\n",
      "Total Reward: 61.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 63\n",
      "Total Reward: 107.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 64\n",
      "Total Reward: 112.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 65\n",
      "Total Reward: 151.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 66\n",
      "Total Reward: 148.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 67\n",
      "Total Reward: 168.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 68\n",
      "Total Reward: 117.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 69\n",
      "Total Reward: 200.0\n",
      "Solved: True\n",
      "\n",
      "Episode: 70\n",
      "Total Reward: 166.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 71\n",
      "Total Reward: 150.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 72\n",
      "Total Reward: 200.0\n",
      "Solved: True\n",
      "\n",
      "Episode: 73\n",
      "Total Reward: 157.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 74\n",
      "Total Reward: 168.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 75\n",
      "Total Reward: 184.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 76\n",
      "Total Reward: 161.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 77\n",
      "Total Reward: 45.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 78\n",
      "Total Reward: 200.0\n",
      "Solved: True\n",
      "\n",
      "Episode: 79\n",
      "Total Reward: 95.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 80\n",
      "Total Reward: 176.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 81\n",
      "Total Reward: 47.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 82\n",
      "Total Reward: 147.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 83\n",
      "Total Reward: 134.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 84\n",
      "Total Reward: 96.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 85\n",
      "Total Reward: 196.0\n",
      "Solved: True\n",
      "\n",
      "Episode: 86\n",
      "Total Reward: 116.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 87\n",
      "Total Reward: 137.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 88\n",
      "Total Reward: 66.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 89\n",
      "Total Reward: 64.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 90\n",
      "Total Reward: 151.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 91\n",
      "Total Reward: 152.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 92\n",
      "Total Reward: 200.0\n",
      "Solved: True\n",
      "\n",
      "Episode: 93\n",
      "Total Reward: 107.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 94\n",
      "Total Reward: 200.0\n",
      "Solved: True\n",
      "\n",
      "Episode: 95\n",
      "Total Reward: 200.0\n",
      "Solved: True\n",
      "\n",
      "Episode: 96\n",
      "Total Reward: 172.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 97\n",
      "Total Reward: 134.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 98\n",
      "Total Reward: 121.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 99\n",
      "Total Reward: 97.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 100\n",
      "Total Reward: 131.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 101\n",
      "Total Reward: 101.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 102\n",
      "Total Reward: 115.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 103\n",
      "Total Reward: 96.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 104\n",
      "Total Reward: 119.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 105\n",
      "Total Reward: 89.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 106\n",
      "Total Reward: 67.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 107\n",
      "Total Reward: 115.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 108\n",
      "Total Reward: 57.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 109\n",
      "Total Reward: 34.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 110\n",
      "Total Reward: 67.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 111\n",
      "Total Reward: 53.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 112\n",
      "Total Reward: 103.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 113\n",
      "Total Reward: 117.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 114\n",
      "Total Reward: 110.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 115\n",
      "Total Reward: 106.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 116\n",
      "Total Reward: 108.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 117\n",
      "Total Reward: 168.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 118\n",
      "Total Reward: 200.0\n",
      "Solved: True\n",
      "\n",
      "Episode: 119\n",
      "Total Reward: 200.0\n",
      "Solved: True\n",
      "\n",
      "Episode: 120\n",
      "Total Reward: 29.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 121\n",
      "Total Reward: 200.0\n",
      "Solved: True\n",
      "\n",
      "Episode: 122\n",
      "Total Reward: 200.0\n",
      "Solved: True\n",
      "\n",
      "Episode: 123\n",
      "Total Reward: 200.0\n",
      "Solved: True\n",
      "\n",
      "Episode: 124\n",
      "Total Reward: 200.0\n",
      "Solved: True\n",
      "\n",
      "Episode: 125\n",
      "Total Reward: 200.0\n",
      "Solved: True\n",
      "\n",
      "Episode: 126\n",
      "Total Reward: 200.0\n",
      "Solved: True\n",
      "\n",
      "Episode: 127\n",
      "Total Reward: 200.0\n",
      "Solved: True\n",
      "\n",
      "Episode: 128\n",
      "Total Reward: 200.0\n",
      "Solved: True\n",
      "\n",
      "Episode: 129\n",
      "Total Reward: 200.0\n",
      "Solved: True\n",
      "\n",
      "Episode: 130\n",
      "Total Reward: 200.0\n",
      "Solved: True\n",
      "\n",
      "Episode: 131\n",
      "Total Reward: 200.0\n",
      "Solved: True\n",
      "\n",
      "Episode: 132\n",
      "Total Reward: 200.0\n",
      "Solved: True\n",
      "\n",
      "Episode: 133\n",
      "Total Reward: 200.0\n",
      "Solved: True\n",
      "\n",
      "Episode: 134\n",
      "Total Reward: 200.0\n",
      "Solved: True\n",
      "\n",
      "Episode: 135\n",
      "Total Reward: 200.0\n",
      "Solved: True\n",
      "\n",
      "Episode: 136\n",
      "Total Reward: 200.0\n",
      "Solved: True\n",
      "\n",
      "Episode: 137\n",
      "Total Reward: 104.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 138\n",
      "Total Reward: 200.0\n",
      "Solved: True\n",
      "\n",
      "Episode: 139\n",
      "Total Reward: 200.0\n",
      "Solved: True\n",
      "\n",
      "Episode: 140\n",
      "Total Reward: 200.0\n",
      "Solved: True\n",
      "\n",
      "Episode: 141\n",
      "Total Reward: 200.0\n",
      "Solved: True\n",
      "\n",
      "Episode: 142\n",
      "Total Reward: 200.0\n",
      "Solved: True\n",
      "\n",
      "Episode: 143\n",
      "Total Reward: 200.0\n",
      "Solved: True\n",
      "\n",
      "Episode: 144\n",
      "Total Reward: 200.0\n",
      "Solved: True\n",
      "\n",
      "Episode: 145\n",
      "Total Reward: 200.0\n",
      "Solved: True\n",
      "\n",
      "Episode: 146\n",
      "Total Reward: 200.0\n",
      "Solved: True\n",
      "\n",
      "Episode: 147\n",
      "Total Reward: 65.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 148\n",
      "Total Reward: 200.0\n",
      "Solved: True\n",
      "\n",
      "Episode: 149\n",
      "Total Reward: 15.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 150\n",
      "Total Reward: 200.0\n",
      "Solved: True\n",
      "\n",
      "Episode: 151\n",
      "Total Reward: 200.0\n",
      "Solved: True\n",
      "\n",
      "Episode: 152\n",
      "Total Reward: 188.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 153\n",
      "Total Reward: 30.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 154\n",
      "Total Reward: 155.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 155\n",
      "Total Reward: 19.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 156\n",
      "Total Reward: 21.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 157\n",
      "Total Reward: 22.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 158\n",
      "Total Reward: 103.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 159\n",
      "Total Reward: 22.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 160\n",
      "Total Reward: 19.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 161\n",
      "Total Reward: 20.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 162\n",
      "Total Reward: 16.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 163\n",
      "Total Reward: 26.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 164\n",
      "Total Reward: 34.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 165\n",
      "Total Reward: 19.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 166\n",
      "Total Reward: 22.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 167\n",
      "Total Reward: 18.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 168\n",
      "Total Reward: 25.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 169\n",
      "Total Reward: 19.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 170\n",
      "Total Reward: 27.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 171\n",
      "Total Reward: 25.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 172\n",
      "Total Reward: 20.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 173\n",
      "Total Reward: 29.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 174\n",
      "Total Reward: 26.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 175\n",
      "Total Reward: 20.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 176\n",
      "Total Reward: 21.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 177\n",
      "Total Reward: 20.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 178\n",
      "Total Reward: 16.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 179\n",
      "Total Reward: 22.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 180\n",
      "Total Reward: 19.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 181\n",
      "Total Reward: 25.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 182\n",
      "Total Reward: 21.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 183\n",
      "Total Reward: 23.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 184\n",
      "Total Reward: 19.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 185\n",
      "Total Reward: 25.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 186\n",
      "Total Reward: 28.0\n",
      "Solved: False\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 187\n",
      "Total Reward: 16.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 188\n",
      "Total Reward: 15.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 189\n",
      "Total Reward: 18.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 190\n",
      "Total Reward: 20.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 191\n",
      "Total Reward: 24.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 192\n",
      "Total Reward: 30.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 193\n",
      "Total Reward: 20.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 194\n",
      "Total Reward: 23.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 195\n",
      "Total Reward: 31.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 196\n",
      "Total Reward: 24.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 197\n",
      "Total Reward: 20.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 198\n",
      "Total Reward: 19.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 199\n",
      "Total Reward: 28.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 200\n",
      "Total Reward: 23.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 201\n",
      "Total Reward: 25.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 202\n",
      "Total Reward: 26.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 203\n",
      "Total Reward: 20.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 204\n",
      "Total Reward: 21.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 205\n",
      "Total Reward: 34.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 206\n",
      "Total Reward: 24.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 207\n",
      "Total Reward: 22.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 208\n",
      "Total Reward: 24.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 209\n",
      "Total Reward: 24.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 210\n",
      "Total Reward: 26.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 211\n",
      "Total Reward: 30.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 212\n",
      "Total Reward: 26.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 213\n",
      "Total Reward: 27.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 214\n",
      "Total Reward: 35.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 215\n",
      "Total Reward: 29.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 216\n",
      "Total Reward: 24.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 217\n",
      "Total Reward: 24.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 218\n",
      "Total Reward: 35.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 219\n",
      "Total Reward: 25.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 220\n",
      "Total Reward: 36.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 221\n",
      "Total Reward: 30.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 222\n",
      "Total Reward: 28.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 223\n",
      "Total Reward: 46.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 224\n",
      "Total Reward: 21.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 225\n",
      "Total Reward: 29.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 226\n",
      "Total Reward: 30.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 227\n",
      "Total Reward: 27.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 228\n",
      "Total Reward: 26.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 229\n",
      "Total Reward: 47.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 230\n",
      "Total Reward: 37.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 231\n",
      "Total Reward: 44.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 232\n",
      "Total Reward: 28.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 233\n",
      "Total Reward: 34.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 234\n",
      "Total Reward: 44.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 235\n",
      "Total Reward: 30.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 236\n",
      "Total Reward: 27.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 237\n",
      "Total Reward: 55.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 238\n",
      "Total Reward: 46.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 239\n",
      "Total Reward: 52.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 240\n",
      "Total Reward: 37.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 241\n",
      "Total Reward: 28.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 242\n",
      "Total Reward: 41.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 243\n",
      "Total Reward: 37.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 244\n",
      "Total Reward: 54.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 245\n",
      "Total Reward: 47.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 246\n",
      "Total Reward: 41.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 247\n",
      "Total Reward: 54.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 248\n",
      "Total Reward: 47.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 249\n",
      "Total Reward: 45.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 250\n",
      "Total Reward: 85.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 251\n",
      "Total Reward: 99.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 252\n",
      "Total Reward: 47.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 253\n",
      "Total Reward: 97.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 254\n",
      "Total Reward: 98.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 255\n",
      "Total Reward: 114.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 256\n",
      "Total Reward: 67.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 257\n",
      "Total Reward: 99.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 258\n",
      "Total Reward: 107.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 259\n",
      "Total Reward: 113.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 260\n",
      "Total Reward: 125.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 261\n",
      "Total Reward: 104.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 262\n",
      "Total Reward: 105.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 263\n",
      "Total Reward: 105.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 264\n",
      "Total Reward: 108.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 265\n",
      "Total Reward: 114.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 266\n",
      "Total Reward: 114.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 267\n",
      "Total Reward: 118.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 268\n",
      "Total Reward: 141.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 269\n",
      "Total Reward: 116.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 270\n",
      "Total Reward: 131.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 271\n",
      "Total Reward: 130.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 272\n",
      "Total Reward: 118.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 273\n",
      "Total Reward: 123.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 274\n",
      "Total Reward: 114.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 275\n",
      "Total Reward: 113.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 276\n",
      "Total Reward: 118.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 277\n",
      "Total Reward: 129.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 278\n",
      "Total Reward: 119.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 279\n",
      "Total Reward: 116.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 280\n",
      "Total Reward: 106.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 281\n",
      "Total Reward: 104.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 282\n",
      "Total Reward: 102.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 283\n",
      "Total Reward: 106.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 284\n",
      "Total Reward: 119.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 285\n",
      "Total Reward: 100.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 286\n",
      "Total Reward: 104.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 287\n",
      "Total Reward: 107.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 288\n",
      "Total Reward: 120.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 289\n",
      "Total Reward: 115.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 290\n",
      "Total Reward: 123.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 291\n",
      "Total Reward: 109.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 292\n",
      "Total Reward: 110.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 293\n",
      "Total Reward: 125.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 294\n",
      "Total Reward: 117.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 295\n",
      "Total Reward: 121.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 296\n",
      "Total Reward: 134.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 297\n",
      "Total Reward: 123.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 298\n",
      "Total Reward: 141.0\n",
      "Solved: False\n",
      "\n",
      "Episode: 299\n",
      "Total Reward: 125.0\n",
      "Solved: False\n",
      "\n"
     ]
    }
   ],
   "source": [
    "main(gamma=0.95)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "![](https://miro.medium.com/max/758/1*bthNFouB07_-ZXzw1N0SSw.gif)\n",
    "The following is a popup window of the cart pole environment being executed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So each time the pole sets back to the middle part it's failing. So from episode 0 until solved= True, it's trying to learn,according to gradient ascent, the best weights on the policy network that maps the states to actions that are giving it the most reward. In this notebook I run 300 episodes , and thus on average the rewards increases and then balances out much better than it did when it originally started, so this tell us that it's learning. In my case the pole balances out first at episode 87 with a total of 200.0 rewards and this is with a gamma choice (scaling factor) at 0.99. I consider a  different scaling factor, 0.95, to see how it behaved. The results show that it had an easier time learning at first because the pole balanced out much quicker , with  it first being solved at episode 47 with a total of 200.0 rewards but utimately at episode 299, it did solve the task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://beta.openai.com\n",
    "\n",
    "Ashish. Understanding OpenAI Gym. Medium, 25 May 2018, https://medium.com/@ashish_fagna/understanding-openai-gym-25c79c06eccb.\n",
    "\n",
    "Osiski, Baej, and Konrad Budek. What is reinforcement learning? The complete guide. Deepsense.Ai, 7 May 2018, https://deepsense.ai/what-is-reinforcement-learning-the-complete-guide.\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
